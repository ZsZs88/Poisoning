{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning Poisonong Attacks in Malware Detection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prestart\n",
    "### Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Files\n",
    "Store the paths for our files in variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "file_arm_malware = \"files\\\\int_malware_arm.csv\"\n",
    "file_arm_benign = \"files\\\\int_benign_arm.csv\"\n",
    "file_mips_malware = \"files\\\\int_malware_mips.csv\"\n",
    "file_mips_benign = \"files\\\\int_benign_mips.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Criterias\n",
    "When reading the datasets, we will need criterias to separate our datasets into smaller portions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def criteria_clean_training(x):\n",
    "    return x % 20 >= 10 or x % 20 == 0 or x % 20 == 5\n",
    "\n",
    "\n",
    "def criteria_poisoned_training(x):\n",
    "    return x % 20 < 10 or x % 20 == 10 or x % 20 == 15\n",
    "\n",
    "\n",
    "def criteria_clean_validation(x):\n",
    "    return x % 20 != 0\n",
    "\n",
    "\n",
    "def criteria_poisoned_validation(x):\n",
    "    return x % 20 != 5\n",
    "\n",
    "\n",
    "def criteria_clean_test(x):\n",
    "    return x % 20 != 10\n",
    "\n",
    "\n",
    "def criteria_poisoned_test(x):\n",
    "    return x % 20 != 15"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the arm datasets\n",
    "$ 8:1:1 $ - Training:Test:Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean dataset\n",
    "For the first model - we call it *Clean* - we will train on clean a clean dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZsZs\\AppData\\Local\\Temp\\ipykernel_25440\\1137577125.py:40: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset_clean_training_arm = dataset_clean_training_arm_malware.append(dataset_clean_training_arm_benign,\n",
      "C:\\Users\\ZsZs\\AppData\\Local\\Temp\\ipykernel_25440\\1137577125.py:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset_clean_validation_arm = dataset_clean_validation_arm_malware.append(dataset_clean_validation_arm_benign,\n",
      "C:\\Users\\ZsZs\\AppData\\Local\\Temp\\ipykernel_25440\\1137577125.py:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset_clean_test_arm = dataset_clean_test_arm_malware.append(dataset_clean_test_arm_benign, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "dataset_clean_training_arm_malware = pd.read_csv(\n",
    "    file_arm_malware,\n",
    "    skiprows=lambda x: criteria_clean_training(x),\n",
    "    index_col=False,\n",
    "    header=None\n",
    ")\n",
    "\n",
    "dataset_clean_validation_arm_malware = pd.read_csv(\n",
    "    file_arm_malware,\n",
    "    skiprows=lambda x: criteria_clean_validation(x),\n",
    "    index_col=False,\n",
    "    header=None\n",
    ")\n",
    "dataset_clean_test_arm_malware = pd.read_csv(\n",
    "    file_arm_malware,\n",
    "    skiprows=lambda x: criteria_clean_test(x),\n",
    "    index_col=False,\n",
    "    header=None\n",
    ")\n",
    "\n",
    "dataset_clean_training_arm_benign = pd.read_csv(\n",
    "    file_arm_benign,\n",
    "    skiprows=lambda x: criteria_clean_training(x),\n",
    "    index_col=False,\n",
    "    header=None\n",
    ")\n",
    "dataset_clean_validation_arm_benign = pd.read_csv(\n",
    "    file_arm_benign,\n",
    "    skiprows=lambda x: criteria_clean_validation(x),\n",
    "    index_col=False,\n",
    "    header=None\n",
    ")\n",
    "dataset_clean_test_arm_benign = pd.read_csv(\n",
    "    file_arm_benign,\n",
    "    skiprows=lambda x: criteria_clean_test(x),\n",
    "    index_col=False,\n",
    "    header=None\n",
    ")\n",
    "\n",
    "dataset_clean_training_arm = dataset_clean_training_arm_malware.append(dataset_clean_training_arm_benign,\n",
    "                                                                       ignore_index=True)\n",
    "dataset_clean_validation_arm = dataset_clean_validation_arm_malware.append(dataset_clean_validation_arm_benign,\n",
    "                                                                           ignore_index=True)\n",
    "dataset_clean_test_arm = dataset_clean_test_arm_malware.append(dataset_clean_test_arm_benign, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Labels for the clean datasets\n",
    "Creating the labels for the clean datasets\n",
    "*Malware* - $ 0 $, *Benign* - $ 1 $\n",
    "-**Trainining**: 800 malware, 800 benign\n",
    "-**Validation**: 100 malware, 100 benign\n",
    "-**Test**: 100 malware, 100 benign"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "labels_clean_training_arm = ([0] * 800) + ([1] * 800)\n",
    "labels_clean_validation_arm = ([0] * 100) + ([1] * 100)\n",
    "labels_clean_test_arm = ([0] * 100) + ([1] * 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Shuffle the *clean* dataframe\n",
    "First we append the labels to the Dataframe, then shuffle the whole Dataframe and finally separate the labels from the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[-1] not found in axis'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [6], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m labels_clean_validation_arm \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(dataset_clean_validation_arm[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     11\u001B[0m labels_clean_test_arm \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(dataset_clean_test_arm[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m---> 13\u001B[0m dataset_clean_training_arm \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(dataset_clean_training_arm\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]))\n\u001B[0;32m     14\u001B[0m dataset_clean_validation_arm \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(dataset_clean_validation_arm\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n\u001B[0;32m     15\u001B[0m dataset_clean_test_arm \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(dataset_clean_test_arm\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[0;32m    310\u001B[0m     )\n\u001B[1;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\pandas\\core\\frame.py:4954\u001B[0m, in \u001B[0;36mDataFrame.drop\u001B[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[0;32m   4806\u001B[0m \u001B[38;5;129m@deprecate_nonkeyword_arguments\u001B[39m(version\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, allowed_args\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m   4807\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n\u001B[0;32m   4808\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4815\u001B[0m     errors: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   4816\u001B[0m ):\n\u001B[0;32m   4817\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4818\u001B[0m \u001B[38;5;124;03m    Drop specified labels from rows or columns.\u001B[39;00m\n\u001B[0;32m   4819\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4952\u001B[0m \u001B[38;5;124;03m            weight  1.0     0.8\u001B[39;00m\n\u001B[0;32m   4953\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4954\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4955\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4956\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4957\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4958\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4959\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4960\u001B[0m \u001B[43m        \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4961\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4962\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\pandas\\core\\generic.py:4267\u001B[0m, in \u001B[0;36mNDFrame.drop\u001B[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[0;32m   4265\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m axis, labels \u001B[38;5;129;01min\u001B[39;00m axes\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m   4266\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 4267\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_drop_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4269\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[0;32m   4270\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_inplace(obj)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\pandas\\core\\generic.py:4311\u001B[0m, in \u001B[0;36mNDFrame._drop_axis\u001B[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001B[0m\n\u001B[0;32m   4309\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mdrop(labels, level\u001B[38;5;241m=\u001B[39mlevel, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[0;32m   4310\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 4311\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m \u001B[43maxis\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4312\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mget_indexer(new_axis)\n\u001B[0;32m   4314\u001B[0m \u001B[38;5;66;03m# Case for non-unique axis\u001B[39;00m\n\u001B[0;32m   4315\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6644\u001B[0m, in \u001B[0;36mIndex.drop\u001B[1;34m(self, labels, errors)\u001B[0m\n\u001B[0;32m   6642\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39many():\n\u001B[0;32m   6643\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 6644\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(labels[mask])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in axis\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   6645\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m indexer[\u001B[38;5;241m~\u001B[39mmask]\n\u001B[0;32m   6646\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelete(indexer)\n",
      "\u001B[1;31mKeyError\u001B[0m: '[-1] not found in axis'"
     ]
    }
   ],
   "source": [
    "dataset_clean_training_arm[\"labels\"] = labels_clean_training_arm\n",
    "dataset_clean_validation_arm[\"labels\"] = labels_clean_validation_arm\n",
    "dataset_clean_test_arm[\"labels\"] = labels_clean_test_arm\n",
    "\n",
    "dataset_clean_training_arm = dataset_clean_training_arm.sample(frac=1)\n",
    "dataset_clean_validation_arm = dataset_clean_validation_arm.sample(frac=1)\n",
    "dataset_clean_test_arm = dataset_clean_test_arm.sample(frac=1)\n",
    "\n",
    "labels_clean_training_arm = np.asarray(dataset_clean_training_arm[\"labels\"])\n",
    "labels_clean_validation_arm = np.asarray(dataset_clean_validation_arm[\"labels\"])\n",
    "labels_clean_test_arm = np.asarray(dataset_clean_test_arm[\"labels\"])\n",
    "\n",
    "dataset_clean_training_arm = np.asarray(dataset_clean_training_arm.drop(columns=[\"labels\"]))\n",
    "dataset_clean_validation_arm = np.asarray(dataset_clean_validation_arm.drop(columns=[\"labels\"]))\n",
    "dataset_clean_test_arm = np.asarray(dataset_clean_test_arm.drop(columns=[\"labels\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Poisoned dataset\n",
    "For the second model - we call it *Poisoned* - we will train on clean a poisoned dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_poisoned_training_arm_malware = pd.read_csv(\n",
    "    file_arm_malware,\n",
    "    skiprows=lambda x: criteria_poisoned_training(x),\n",
    "    index_col=False,\n",
    "    header=None\n",
    ")\n",
    "dataset_poisoned_validation_arm_malware = pd.read_csv(\n",
    "    file_arm_malware,\n",
    "    skiprows=lambda x: criteria_poisoned_validation(x),\n",
    "    index_col=False,\n",
    "    header=None\n",
    ")\n",
    "dataset_poisoned_test_arm_malware = pd.read_csv(\n",
    "    file_arm_malware,\n",
    "    skiprows=lambda x: criteria_poisoned_test(x),\n",
    "    index_col=False,\n",
    "    header=None\n",
    ")\n",
    "\n",
    "dataset_poisoned_training_arm_benign = pd.read_csv(\n",
    "    file_arm_benign,\n",
    "    skiprows=lambda x: criteria_poisoned_training(x),\n",
    "    index_col=False,\n",
    "    header=None\n",
    ")\n",
    "dataset_poisoned_validation_arm_benign = pd.read_csv(\n",
    "    file_arm_benign,\n",
    "    skiprows=lambda x: criteria_poisoned_validation(x),\n",
    "    index_col=False,\n",
    "    header=None\n",
    ")\n",
    "dataset_poisoned_test_arm_benign = pd.read_csv(\n",
    "    file_arm_benign,\n",
    "    skiprows=lambda x: criteria_poisoned_test(x),\n",
    "    index_col=False,\n",
    "    header=None\n",
    ")\n",
    "\n",
    "dataset_poisoned_training_arm = dataset_poisoned_training_arm_malware.append(dataset_poisoned_training_arm_benign,\n",
    "                                                                             ignore_index=True)\n",
    "dataset_poisoned_validation_arm = dataset_poisoned_validation_arm_malware.append(dataset_poisoned_validation_arm_benign,\n",
    "                                                                                 ignore_index=True)\n",
    "dataset_poisoned_test_arm = dataset_poisoned_test_arm_malware.append(dataset_poisoned_test_arm_benign,\n",
    "                                                                     ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Labels for the poisoned datasets\n",
    "Creating the labels for the poisoned datasets\n",
    "*Malware* - $ 0 $, *Benign* - $ 1 $\n",
    "-**Trainining**: 800 malware, 800 benign\n",
    "-**Validation**: 100 malware, 100 benign\n",
    "-**Test**: 100 malware, 100 benign"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_poisoned_training_arm = ([0] * 800) + ([1] * 800)\n",
    "labels_poisoned_validation_arm = ([0] * 100) + ([1] * 100)\n",
    "labels_poisoned_test_arm = ([0] * 100) + ([1] * 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Shuffle the *poisoned* dataframe\n",
    "First we append the labels to the Dataframe, then shuffle the whole Dataframe and finally separate the labels from the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_poisoned_training_arm[\"labels\"] = labels_poisoned_training_arm\n",
    "dataset_poisoned_validation_arm[\"labels\"] = labels_poisoned_validation_arm\n",
    "dataset_poisoned_test_arm[\"labels\"] = labels_poisoned_test_arm\n",
    "\n",
    "dataset_poisoned_training_arm = dataset_poisoned_training_arm.sample(frac=1)\n",
    "dataset_poisoned_validation_arm = dataset_poisoned_validation_arm.sample(frac=1)\n",
    "dataset_poisoned_test_arm = dataset_poisoned_test_arm.sample(frac=1)\n",
    "\n",
    "labels_poisoned_training_arm = np.asarray(dataset_poisoned_training_arm[\"labels\"])\n",
    "labels_poisoned_validation_arm = np.asarray(dataset_poisoned_validation_arm[\"labels\"])\n",
    "labels_poisoned_test_arm = np.asarray(dataset_poisoned_test_arm[\"labels\"])\n",
    "\n",
    "dataset_poisoned_training_arm = np.asarray(dataset_poisoned_training_arm.drop(columns=[\"labels\"]))\n",
    "dataset_poisoned_validation_arm = np.asarray(dataset_poisoned_validation_arm.drop(columns=[\"labels\"]))\n",
    "dataset_poisoned_test_arm = np.asarray(dataset_poisoned_test_arm.drop(columns=[\"labels\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models for arm Datasets\n",
    "In this section we will build the models with the exact same struture\n",
    "Neural network with one *hidden layer* with a $sigmoid$ *activation function*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model for Clean Data\n",
    "Building, fitting and evaluating the Model with the *clean* datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_clean = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(1, input_shape=(131,), activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "model_clean.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "model_clean.fit(dataset_clean_training_arm, labels_clean_training_arm, epochs=10,\n",
    "                validation_data=(dataset_clean_validation_arm, labels_clean_validation_arm))\n",
    "\n",
    "model_clean.evaluate(dataset_clean_test_arm, labels_clean_test_arm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model for Poisoned Data\n",
    "Building, fitting and evaluating the Model with the *poisoned* datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_poisoned = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(1, input_shape=(131,), activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "model_poisoned.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                       metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "model_poisoned.fit(dataset_poisoned_training_arm, labels_poisoned_training_arm, epochs=10,\n",
    "                   validation_data=(dataset_poisoned_validation_arm, labels_poisoned_validation_arm))\n",
    "\n",
    "model_poisoned.evaluate(dataset_poisoned_test_arm, labels_poisoned_test_arm)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
